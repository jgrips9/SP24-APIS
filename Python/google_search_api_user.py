# -*- coding: utf-8 -*-
"""Google Search API User.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EjuBMe-R8PacQwkAjgXDhrmg8FW6vp5S
"""

import requests
import re
import pandas as pd
#Input your credentials below
API_KEY = ''
SEARCH_ENGINE_ID = ''

search_query = 'NeuralNine books'

url = 'https://www.googleapis.com/customsearch/v1'
params = {
'q': search_query,
'key': API_KEY,
'cx': SEARCH_ENGINE_ID
    }

#List of parameters to include
#https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list

response = requests.get(url, params = params)
results = response.json()
print(results)

#Printing the link of the first result
if 'items' in results:
    print(results['items'][0]['link'])

#now doing images.
search_query = 'cats'

url = 'https://www.googleapis.com/customsearch/v1'
params = {
'q': search_query,
'key': API_KEY,
'cx': SEARCH_ENGINE_ID,
'searchType': 'image',
'dateRestrict': '2022-01-01:2020-12-31'
    }

response = requests.get(url, params = params)
results = response.json()['items']
print(results)

for item in results:
    print(item['link'])

#Now doing pdfs.
search_query = 'python tutorial'
start_p = 1
url = 'https://www.googleapis.com/customsearch/v1'
params = {
'q': search_query,
'key': API_KEY,
'cx': SEARCH_ENGINE_ID,
'fileType': 'pdf',
'num': 10,
'start': start_p
    }

response = requests.get(url, params = params)
results = response.json()['items']
print(results)

for item in results:
    print(item['link'])

print(response.json())

results = response.json()

#Limited to 10 results per request.
len(results['items'])

#Print information of first result.
results['items'][0]

#Figure out a way to get more than 10 results in googl api search. Maybe go to the next page?
#Now doing pdfs.
search_query = 'python tutorial'
start_p = start_p + 10
url = 'https://www.googleapis.com/customsearch/v1'
params = {
'q': search_query,
'key': API_KEY,
'cx': SEARCH_ENGINE_ID,
'fileType': 'pdf',
'start': start_p
    }

response = requests.get(url, params = params)
results = response.json()['items']
print(results)

for item in results:
    print(item['link'])

#Now incorporate into loop. to get several results.
start_p = 1
search_query = 'python tutorial'
url = 'https://www.googleapis.com/customsearch/v1'

i = 1
for j in range(3):
  #Adjust paramaters
  params = {
  'q': search_query,
  'key': API_KEY,
  'cx': SEARCH_ENGINE_ID,
  'fileType': 'pdf',
  'start': start_p
    }
  response = requests.get(url, params = params)
  results = response.json()['items']
  for item in results:
    print(item['link'])
  start_p = i*10+1
  i = i+1

#Now extract certain information that I want available to me from the API. Store into a data frame. 'htmlTitle', 'link', 'snippet' 'pagemap'(creationdate, creator, author)
title = []
link = []
snippet = []
date = []
creator = []
author = []

for item in results:
  title.append(item['title'])
  link.append(item['link'])
  snippet.append(item['snippet'])
  if "creationdate" in item['pagemap']['metatags'][0]:
    date.append(item['pagemap']['metatags'][0]['creationdate'])
  else:
    date.append('Missing')
  if "creator" in item['pagemap']['metatags'][0]:
    creator.append(item['pagemap']['metatags'][0]['creator'])
  else:
    creator.append("missing")
  if "author" in item['pagemap']['metatags'][0]:
    author.append(item['pagemap']['metatags'][0]['author'])
  else:
    author.append("missing")

#Now create pandas dataframe
data = pd.DataFrame({
    'title': title,
    'URL': link,
    'TextSnippet': snippet,
    'date': date,
    'creator': creator,
    'author': author
})
display(data)

#Now the same process but through multiple page results.
#Now incorporate into loop. to get several results.
start_p = 1
search_query = 'python tutorial'
url = 'https://www.googleapis.com/customsearch/v1'

title = []
link = []
snippet = []
date = []
creator = []
author = []

i = 1
for j in range(3):
  #Adjust paramaters
  params = {
  'q': search_query,
  'key': API_KEY,
  'cx': SEARCH_ENGINE_ID,
  'fileType': 'pdf',
  'start': start_p
    }
  response = requests.get(url, params = params)
  results = response.json()['items']

  for item in results:
    title.append(item['title'])
    link.append(item['link'])
    snippet.append(item['snippet'])
    if "metatags" in item['pagemap']:
      if "creationdate" in item['pagemap']['metatags'][0]:
        date.append(item['pagemap']['metatags'][0]['creationdate'])
      else:
        date.append('Missing')
      if "creator" in item['pagemap']['metatags'][0]:
        creator.append(item['pagemap']['metatags'][0]['creator'])
      else:
        creator.append("missing")
      if "author" in item['pagemap']['metatags'][0]:
        author.append(item['pagemap']['metatags'][0]['author'])
      else:
        author.append("missing")
    else:
      date.append('Missing')
      creator.append("missing")
      author.append("missing")
  start_p = i*10+1
  i = i+1

#Create pandas dataframe
data = pd.DataFrame({
    'title': title,
    'URL': link,
    'TextSnippet': snippet,
    'date': date,
    'creator': creator,
    'author': author
})
display(data)